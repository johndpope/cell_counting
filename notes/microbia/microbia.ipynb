{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload our package (for use if modified).\n",
    "import importlib\n",
    "importlib.reload(load)\n",
    "importlib.reload(normalize)\n",
    "importlib.reload(process)\n",
    "importlib.reload(batch)\n",
    "importlib.reload(cnn)\n",
    "importlib.reload(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the project's root directory on our PYTHONPATH.\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from our package.\n",
    "from src.preprocessing import load, normalize, process, batch\n",
    "from src.classification import cnn, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from other libraries.\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of our net as a list of layers.\n",
    "layers = [\n",
    "    cnn.ConvolutionalLayer(5, 5, 1, \"VALID\", \"convlayer\"),\n",
    "    cnn.TransferLayer(lambda x, name: tf.sigmoid(x, name=name), \"trans1layer\"),\n",
    "    cnn.ConvolutionalLayer(20, 3, 1, \"VALID\", \"conv2layer\"),\n",
    "    cnn.TransferLayer(lambda x, name: tf.sigmoid(x, name=name), \"trans2layer\"),\n",
    "    cnn.FlatLayer(\"flatlayer\"),\n",
    "    cnn.FullLayer(100, \"full1layer\"),\n",
    "    cnn.TransferLayer(lambda x, name: tf.sigmoid(x, name=name), \"trans3layer\"),\n",
    "    cnn.FullLayer(100, \"full2layer\"),\n",
    "    cnn.TransferLayer(lambda x, name: tf.sigmoid(x, name=name), \"trans4layer\"),\n",
    "    cnn.FullLayer(2, \"full3layer\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_session_config': None, '_tf_random_seed': 1, '_save_summary_steps': 100, '_model_dir': 'models/13'}\n",
      "WARNING:tensorflow:Estimator's model_fn (<bound method Model._model_fn of <src.classification.cnn.NeuralNet object at 0x7fa1a259e2b0>>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "# Construct the net.\n",
    "sess = tf.Session()\n",
    "net = cnn.NeuralNet(\n",
    "    layers,\n",
    "    lambda labels, predictions: tf.losses.softmax_cross_entropy(\n",
    "        labels,\n",
    "        predictions,\n",
    "        weights=1,\n",
    "        label_smoothing=0,\n",
    "        scope=None,\n",
    "        loss_collection=tf.GraphKeys.LOSSES,\n",
    "        reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS\n",
    "    ),\n",
    "    {\n",
    "        \"accuracy\": metrics.accuracy,\n",
    "        \"false_negative_rate\": metrics.false_negative_rate,\n",
    "        \"false_positive_rate\": metrics.false_positive_rate,\n",
    "        \"true_positive_rate\": metrics.true_positive_rate,\n",
    "        \"true_negative_rate\": metrics.true_negative_rate,\n",
    "        \"roc_auc\": metrics.roc_auc\n",
    "    },\n",
    "    lambda: tf.train.MomentumOptimizer(0.01, 0.5),\n",
    "    \"models/\" + \"14\",\n",
    "    \"nn\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data.\n",
    "train_images, train_masks = load.load_images_and_masks(\"data/train\", [str(i) for i in range(900)])\n",
    "train_images = normalize.smdm_normalize(train_images, 21, \"REFLECT\")\n",
    "train_patches, train_classes = process.patchify(train_images, train_masks, 21, 4)\n",
    "train_classes = process.one_hot_encode(train_classes)\n",
    "train_input_fn = batch.input_fn(train_patches, train_classes, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the net for 5 minutes.\n",
    "with tf.Session().as_default():\n",
    "    net.train_duration_(train_input_fn, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data.\n",
    "test_images, test_masks = load.load_images_and_masks(\"data/test\", [str(i) for i in range(100)])\n",
    "test_images = normalize.smdm_normalize(test_images, 21, \"REFLECT\")\n",
    "test_patches, test_classes = process.patchify(test_images, test_masks, 21, 4)\n",
    "test_classes = process.one_hot_encode(test_classes)\n",
    "test_input_fn = batch.input_fn(test_patches, test_classes, 5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the net.\n",
    "metric_dict = net.evaluate_(test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the test results.\n",
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
